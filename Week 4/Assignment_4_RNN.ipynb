{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "Yeah ! Let's start with our actual project. In this assignment we will load the database and do preprocessing tasks.\n",
    "Ensure you have following packages installed\n",
    "1. numpy\n",
    "2. pandas  \n",
    "( Hope you are familiar with above two modules well )\n",
    "3. nltk (don't worry, we just need this to remove stopwords while preprocessing)\n",
    "4. tensorflow\n",
    "5. keras\n",
    "6. scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saart\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Importing essential libraries and functions\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import json\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the IMDB Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review       probably my all-time favorite movie, a story o...\n",
      "sentiment                                             positive\n",
      "Name: 5, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "'''\n",
    "Using pandas load the imdb reviews csv file\n",
    "Analyse its shape and columns\n",
    "Check for missing data and remove those rows\n",
    "'''\n",
    "df = pd.read_csv(\"IMDB_Dataset.csv\")\n",
    "df.dropna(inplace = True)\n",
    "df['review'] = df['review'].str.lower()\n",
    "print(df.loc[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Complete the function to preprocess the text data\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    # First make the sentence lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Remove all html tags from the sentence i.e replace anything between <> with space\n",
    "    # Hint use Regular Expression i.e. re.sub()\n",
    "    sentence = re.sub(r'<.*?>',\" \",sentence)\n",
    "    \n",
    "    \n",
    "    # Remove all special characters i.e. anything other than alphabets and numbers. Replace them with space\n",
    "    special = r'[^a-zA-Z0-9\\s]'\n",
    "    sentence = re.sub(special,\" \",sentence)\n",
    "    # Remove all single characters i.e. a-z and A-Z and Replace them with space\n",
    "    single = r'\\b[a-zA-Z]\\b'\n",
    "    sentence = re.sub(single,\" \",sentence)\n",
    "    # Remove all multiple spaces and replace them with single space\n",
    "    sentence = re.sub(\"\\s+\",\" \",sentence)\n",
    "    # Use the nltk library to remove all stopwords from the sentence\n",
    "    # stopwords are the words like and, the, is, are etc.\n",
    "    words = word_tokenize(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentence = [word for word in words if word not in stop_words]\n",
    "    sentence = ' '.join(sentence)\n",
    "\n",
    "       \n",
    "    # return the sentence\n",
    "    return(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :\n",
    "# Call the preprocessing function for each review in the dataframe and\n",
    "# save the results in a new list of preprocessed_reviews\n",
    "# This list will be your input to the neural network\n",
    "# We will call this list as X from now on\n",
    "df['preprocessed_reviews'] = df['review'].apply(preprocessing)\n",
    "X = df['preprocessed_reviews'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO :\n",
    "# Convert sentiment column in the dataframe to numbers\n",
    "# Convert positive to 1 and negative to 0 and store it in numpy array\n",
    "# We will call this numpy array as y from now on\n",
    "df['sentiment_number'] = df['sentiment'].map({'positive': 1,'negative': 0})\n",
    "y = df['sentiment_number'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Split the data into training and testing (80-20 ratio)\n",
    "# The train set will be used to train our deep learning models \n",
    "# while test set will be used to evaluate how well our model performs \n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing embedding layer\n",
    "Let's now write the script for our embedding layer. Embedding layer converts our textual data into numeric form. It is then **used as the first layer for the deep learning model like LSTM**.  \n",
    "To know more about word embedding you may refer to following video\n",
    "https://www.youtube.com/watch?v=9S0-OC4LFNo  \n",
    "#### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92185"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "word_tokenizer = Tokenizer()\n",
    "\n",
    "# TODO: Fit the tokenizer on the training data (X_train)\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# TODO: Convert training data to sequences of integers\n",
    "# Hint: Use texts_to_sequences method\n",
    "X_train_sequences = word_tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# TODO: Convert test data to sequences of integers\n",
    "# Hint: Use texts_to_sequences method\n",
    "X_test_sequences = word_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# End TODO\n",
    "# Saving the tokenizer in a json file (Already done for you)\n",
    "# This will be used later for prediction on data in next assignments\n",
    "tokenizer_json = word_tokenizer.to_json()\n",
    "with io.open('b3_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    \n",
    "# Vocab_length is the number of unique words in our dataset\n",
    "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding all reviews to be of same length 'maxlen' words\n",
    "maxlen = 100\n",
    "# You can try different dimensions like 50, 100, 200 and 300\n",
    "# and see how the model performs in next week\n",
    "\n",
    "# TODO: Pad the training data sequences\n",
    "# Hint: Use pad_sequences with 'post' padding and maxlen=maxlen\n",
    "X_train_padded = pad_sequences(X_train_sequences,padding = 'post', maxlen = maxlen)\n",
    "\n",
    "# TODO: Pad the test data sequences\n",
    "# Hint: Use pad_sequences with 'post' padding and maxlen=maxlen\n",
    "X_test_padded = pad_sequences(X_test_sequences, padding = 'post', maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the embedding matrix : (92185, 100)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary for embeddings\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "# Open the GloVe file (a2_glove.6B.100d.txt) with utf-8 encoding\n",
    "glove_file = open('glove_embeddings.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "# TODO : Create an embedding matrix where each row corresponds to the index of the\n",
    "# unique word in the dataset and each column corresponds to the word vector\n",
    "# in the GloVe embedding \n",
    "# So the matrix will have vocab_length rows and maxlen columns\n",
    "embedding_matrix = np.zeros((vocab_length,100))\n",
    "for word,index in word_tokenizer.word_index.items():\n",
    "    if index<vocab_length:\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "print(\"Shape of the embedding matrix :\", embedding_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saart\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,218,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m9,218,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,218,500</span> (35.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,218,500\u001b[0m (35.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,218,500</span> (35.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m9,218,500\u001b[0m (35.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 24ms/step - accuracy: 0.5290 - loss: 0.7005 - val_accuracy: 0.5523 - val_loss: 0.6752\n",
      "Epoch 2/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.5440 - loss: 0.6843 - val_accuracy: 0.5540 - val_loss: 0.6714\n",
      "Epoch 3/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.5482 - loss: 0.6803 - val_accuracy: 0.5745 - val_loss: 0.6625\n",
      "Epoch 4/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.5420 - loss: 0.6806 - val_accuracy: 0.5743 - val_loss: 0.6663\n",
      "Epoch 5/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.5277 - loss: 0.6928 - val_accuracy: 0.5450 - val_loss: 0.6856\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5469 - loss: 0.6860\n",
      "Test Loss: 0.6855685710906982\n",
      "Test Accuracy: 0.5450000166893005\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, SpatialDropout1D\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_length,output_dim=100,weights=[embedding_matrix],input_length=maxlen,\n",
    "                    trainable=False))\n",
    "\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(SimpleRNN(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_padded, np.array(y_train),epochs=5, \n",
    "                    batch_size=64,validation_data=(X_test_padded, np.array(y_test)),\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "score = model.evaluate(X_test_padded, np.array(y_test), verbose=1)\n",
    "print(f'Test Loss: {score[0]}')\n",
    "print(f'Test Accuracy: {score[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_analysis_lstm_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
